Our data on soviet repressions come from  \citet{zhukov_stalins_2018}\footnote{In particular, we downloaded the data from the replications file archive of the journal available  at \url{https://www.prio.org/JPR/Datasets/}} who use  the Victims of Political Terror archive \footnote{The Memorial archive can be accessed at \url{http://base.memo.ru/} (new version) or at \url{http://lists.memo.ru/} (older version)} collected by a Russian NGO Memorial. The main sources of the Memorial lists are declassified Russian Interior Ministry documents, prosecutorâ€™s offices and the Commission for the Rehabilitation of Victims of Political Repression.
 The Memorial archives include 2.7 million individual arrests by the Soviet secret police (NKVD) between  the years 1921 and 1959 with names of each person, date of arrest, the place of birth for all observations and  in many cases additional information such as ethnicity, occupation and party membership. 
 However, the data are not complete and include only about 70\% of estimated 3.8  million people convicted under Article 58.

Another major issue with the data is frequent  missingness of information on ethnicity and date of arrests. 
We try to address this issue by imputing missing values using names and date of process. The imputation procedures are described in section 

%\subsection{Missing Data Analysis}
Missing data presents another major challenge. 
%Another major issue with the data is frequent  
 Table \ref{tab:missing_data_count} shows how many values are missing for the variables of our interest. We can see that information on ethnicity is not available for more than half of all observations. In contrast, a surname is recorded for every arrest in the dataset and first name is missing only for negligible fraction of observations. The availability of information on names enables us to use them to infer missing ethnicity of an individual. In particular, we will train a Naive Bayes classifier on the 1 197 373 with known ethnicity and use the model's predictions to impute the ethnicity for the remaining 1 507 177 observations (the details are described in the subsection \ref{subsec:inferring_ethnicity}).
\input{tables/missing_data_count.tex}

Date of arrest, which is also necessary for our analysis, has even higher rate of missingness than ethnicity. 
One solution, albeit only partial, might be to use the date of process (where it is available)  to extrapolate the missing date of arrest.
As is shown in the table \ref{tab:missing_date_of_arrest_process}, we could impute this way the missing date of arrests for 903 493 observations for which we have the date of process. 
Total number of arrests used for our analysis would therefore increase from 1 053 638 to  1 957 131.  However, there remains 747 419 observations for which neither the date of arrest nor the date of process are known. 
%Unfortunately, not much can be done address this problem. 
%The table \ref{tab:missing_date_of_arrest_process} shows for this 
\input{tables/missing_date_of_arrest_process.tex}


We created our main dataset by counting number of arrest for each ethnicity by year-quater.  A few people who were categorized as having multiple ethnicities were dropped from the dataset and not counted. 
With 17  minorities (Armenian, Belorussian, Estonian, German, Greek, Chechen, Chinese, Jewish, Kabardin, Kalmyk, Korean, Latvian, Lithuanian, Ossetian, Polish, Tatar and Ukrainian) and 148 time periods (from 1921 to 1958) this gives us 2652 observations in total. Total number of arrests for each ethnicity is provided in the table \ref{tab:} and the plot of arrest by ethnicity and year (after applying the transformation $\log\left(1 + y_{it}\right)$) is shown in figure \ref{fig:universe}, both in the appendix. 

In addition to data on repressions, we also obtained some information on a few characteristics of the 17 ethnic groups in the USSR. 
Specifically, we acquired total population of the ethnic groups and their urbanization rate from 1926 Soviet Census from the Demoscope website.\footnote{It is available online at \url{http://www.demoscope.ru/weekly/ssp/ussr_nac_26.php}} For each ethnic group, we also calculated the cladisitc similarity of its language to Russian based from Glottolog language trees \citep{hammarstrom_glottolog_2018}.
Cladistic measure of linguistic similarity counts the number of shared branching points between the two nodes on a language tree and it has been used by \citet{fearon_ethnic_2003} and \citet{dickens_ethnolinguistic_2018} among others. 
%For example, Ukrainian and Russian have the cladisitc similarity of 4 since they are both 
The full data are provided in the table \ref{tab:} in the appendix.


\newpage
\section{Imputation of Missing Data} \label{sec:missing_data}
%napis o missing at random, completely at random asumptions
\subsection{Inferring Ethnicity from Names} \label{subsec:inferring_ethnicity}
In this section, we explain our method for predicting ethnicity of an individual from his or her names. Using names for imputing ethnicity has several advantages. First, full name  is available for every individuals in the dataset. Second, names have been shown to be highly predictive of ethnicity in a variety of applications \citep{mateos_review_2007, hofstra_sources_2017, hofstra_predicting_2018}. 

Given the high number of predictors, we need a model that is not  computationally demanding but at the same time achieves reasonable level of prediction accuracy. Naive Bayes classifier meets these criteria and has been for this reason used in wide range of applications including text classification  \citep{gentzkow_text_2019}.
%We will brief introduce its 

\subsubsection{Naive Bayes Classifier}
%In particular, we use Naive Bayes classifier. 
Let  $\boldsymbol{x} = \left(x_1, x_2, x_3\right)$ be features used for predicting ethnicity, that is a person's first, last, and patronymic (given after father's first name) names. Using Bayes theorem, we can express the probability that particular observation  belongs to ethnic group $E_k$ given its features as
\begin{equation}
p(E_k \mid \mathbf{x}) = \frac{p(E_k) \ p(\mathbf{x} \mid E_k)}{p(\mathbf{x})}
\end{equation}
in other words, the posterior probability is proportional to the product of prior probability and likelihood. 
Assuming conditional independence of features allows us to substitute $p(\mathbf{x} \mid E_k)$ such that we get

\begin{equation}
p(E_k \mid \mathbf{x}) = \frac{p(E_k) \  \prod_{i=1}^3 p(x_i \mid E_k)\,}{p(\mathbf{x})}
\end{equation}
%In this context, the conditional independence of features means within each ethnic group having a certain first name does not influence probability of having some surnames and patronymic names   
 All terms in this equation can be estimated from the data: the prior probability $p(E_k)$ as a proportion of $E_k$ in the data, $p(x_i \mid E_k)$ as a proportion of people with name $x_i$ in the ethnic group $E_k$ and $p(\mathbf{x})$ simply calculated such that the sum of $p(E_k \mid \mathbf{x})$ for all $k$ is one. 
%Thus, having a name that is typical for a given ethnicity results in high probabilty
%The Naive Bayes classifier assigns high probability to names that are 
The Naive Bayes classifier then chooses the ethnicity with the highest posterior probability as its prediction
\begin{equation}
    \hat{y} = \underset{k \in \{1, \dots, K\}}{\operatorname{argmax}} \ p(E_k) \displaystyle\prod_{i=1}^{3} p(x_i \mid E_k)
\end{equation}

However, one potential issue is that whenever a likelihood of a certain feature is estimated to be 0 then the posterior probability is always 0 regardless of the prior  or the likelihoods of other features. For example, suppose that a person has a typical German first name but a rare surname which does not appear in the training set at all. 
Then the useful information contained in the first name will be completely ignored since the 0 likelihood of the surname will override any other value and we will end up with the posterior probability of zero for all ethnic groups.

To address this problem, we apply Laplace smoothing. 
For every ethnicity $k$, let $c_j$ be number of people with a name $j$ and $N$ be total number of people.  Without applying any smoothing, we would estimate the likelihood  $p(x_i \mid E_k)$ simply as a relative frequency, i.e. $\hat\theta_j = \frac{c_j}{N}$. With  Laplace smoothing, we estimate  the  likelihood $\hat\theta_j$ as 
\begin{equation}
    \hat\theta_j = \frac{c_j + \alpha}{N + \alpha d} \qquad j = 1, \dots, d
\end{equation}
where parameter $\alpha > 0 $ is a smoothing parameter.  This ensures that for any finite value of $N$,  $\hat\theta_j$ will never be zero. %Nevertheless,  $\alpha$ should not be set too high since that could 




It is important to note that the conditional independence assumption often does not hold in the data and the estimated posterior probabilities therefore have to be taken with a grain of salt. %have are thus not always reliable. 
However, our main goal is the best out-of-sample accuracy of the model's predictions. In this respect, Naive Bayes classifier have been shown to perform  well in many applications, despite its often violated assumptions \citep{domingos_optimality_1997}.

\subsubsection{Adjusting for Unbalanced Prediction Accuracy}
To reliably asses the out-of-sample performance of our model, we used 10-fold cross-validation on the training dataset. That is, the data are first randomly split into 10 groups, Then the model is fit to the data from to the data ..  
Using this method, we get the overall 75.4\% accuracy. Nevertheless, it varies
significantly by ethnicity. The specificity and sensitivity by ethnic group are provided in the table \ref{tab:sens_spec}. Some ethnic groups with distinctive names such as Chinese, Korean and German are classified fairly well with sensitivity higher than 75\%. 

Thus, there are biases in our predictions. However, we can try to adjust for them. Let $P_{it}$ be the number of people with predicted ethnicity $i$ arrested at time $t$, $R_{it}$ be actual the number of people with ethnicity $i$ arrested at time $t$, $\alpha_i$ and $\beta_i$  be sensitivity and specificity of our classifier for ethnic group $i$ and $N_t$ be the total number of arrests at time $t$. Then the predicted arrests of a given ethnicity are sum of true positives and false positives, that is
\begin{equation}
    P_{it} = \alpha_i R_{it} + \left( N_t - R_{it} \right) \cdot \left(1 - \beta_i \right)  
\end{equation}
We are interested in $R_{it}$ but we only directly observe $P_{it}$ and $N_t$. However using simple algebra, $R_{it}$ can be expressed as
\begin{equation} \label{eq:pars_adj}
 R_{it} = \frac{P_{it} - N_t  \left(1 - \beta_i \right)}{\alpha_i + \beta_i - 1}
\end{equation}
We will refer to this method of correcting predictions as parsimonious adjustment.  The parameters $\alpha_i$ and $\beta_i$ are not known to us but we can use their estimates from the cross-validation on the training data. This assumes that the these parameters do not differ significantly for the training and test data. But this might not be the case. 
Suppose, for example, that Armenians are often misclassified as Chechens and that the number of Armenians in the data with missing ethnicity  is disproportionately higher than in the data with information on ethnicity. 
Then the cross-validated specificity for Chechens in the training set will underestimate the specificity in the test set because it does not take into account higher proportion of Armenians. 
%If, for example, the proportions of the arrests of some ethnic group are substantially different between the test and training set and if there is constant then 

Fortunately, we can address this potential bias by more building complex modeling.
First for all ethnic groups $i$ and $j$, we define the misclassification rate $b_{ij}$ as share of people with ethnicity $j$ that are classified as $i$.
Notice that for $i = j$, the misclassification rate is simply prediction accuracy for ethnicity $i$.
It follows from the definition of the terms that predicted number of arrests for ethnic group $i$ at time $t$,  $P_{it}$, is equal to 
\begin{equation}
P_{it}  = \sum_{j = 1}^{K} b_{ij} R_{jt} \qquad i = 1, \dots, K
\end{equation}
This equation can be expressed in matrix form as
\begin{equation}
 \mathbf{P}_t = \mathbf{B} \cdot \mathbf{R}_t 
\end{equation}
where $\mathbf{P}_t = \left(P_{1t}, \cdots, P_{Kt} \right)$, $\mathbf{R}_t = \left(R_{1t}, \cdots, R_{Kt} \right)$, and $\mathbf{B} = \left(b_{ij}\right)_{i = 1, \dots, K,\:j = 1, \dots, K}$.
To express $\mathbf{R}_t$, we just  apply basic linear algebra
\begin{equation}
\mathbf{R}_t  = \mathbf{B}^{-1} \cdot  \mathbf{P}_t
\end{equation}
We will call this method the full matrix adjustment. Compared to the parsimonious adjustment (in equation \ref{eq:pars_adj}), this correction no longer assumes that the test set sensitivity and specificity be accurately estimated  from the training set.  The full matrix adjustment
makes only somewhat weaker assumption that the train and test set misclassification rates are not significantly different. 
%On the other hand, it might be noisier
% lower bias, higher variance


% Jeste se zmin o scalovani

%\begin{equation*}
%\begin{array}{rcl} P_{1t} & = & b_{11} R_{1} \\ 
%f(x,y,z) & = & x + y + z 
%\end{array}
%\end{equation*}



%We get bigger picture by examining the
%confusion matrix provided in the table \ref{tab:conf_matrix_count} in the appendix. The confusion matrix is defined as 

